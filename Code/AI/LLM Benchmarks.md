- **Humanity's Last Exam** extremely hard reasoning benchmark `math, puzzles, paradoxes`
- **GPQA** Graduate-Level Google-Proof QA `hard scientific exams`
- **MMMU** Massive Multimodal Multitask Understanding
  30 subjects `images, diagrams, plots`
- **LiveCodeBench** IDE copilot simulation `writing code, edits, vague prompts`
- **SWE-bench** HumanEval with memorization avoidance `GitHub issues + PRs`
- **AIME** multi-step math reasoning, logical deduction
- **AGIEval** simulates human exams `SAT`, better real world reasoning than `MMLU`
- **MMLU** Massive Multitask Language Understanding
  English QA `multiple choice`, 57 subjects `math, law, med`
- **HumanEval** code generation `python`, functional correctness
- **LMSYS Arena** human preference-based rankings https://lmarena.ai/leaderboard
- **SimpleQA** Short-answer factual QA `unambiguous answers`

Useful for Coding `LiveCodeBench, SWE-bench, HumanEval`
Useful in General `Humanity's Last Exam, GPQA, AGIEval, LMSYS Arena, MMMU, MMLU`
Useful for Vision `MMMU, General`

unlike in [[Self Attention]] where each token attends to itself and other tokens in the same sequence 
here each token attends to token from another sequence 
> [!example] text-token attending to image-tokens
